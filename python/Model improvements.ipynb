{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import boto3\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logfuncts import logger\n",
    "\n",
    "FDIR = os.path.abspath(\".\")\n",
    "DATABASE_PATH = os.path.join(FDIR, '../data.db')\n",
    "OUTPUT_PATH = os.path.join(FDIR, '../html/dart.json')\n",
    "RIVER_NAME = \"dart\"\n",
    "\n",
    "NUM_STEPS = 80\n",
    "NUM_LEVEL_UPDATES = 40\n",
    "MIMIMUM_THRESHOLD = 0.7\n",
    "MAXIMUM_THRESHOLD = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe_from_sql(river, limit=-1):\n",
    "    \"\"\"Load data from the database and return a pandas dataframe. \n",
    "    Limit param specifies number of rows returned. Default is to return all\"\"\"\n",
    "    if limit > 0:\n",
    "        logger.debug(\"loading df for river {river} from sql with row limit of {limit}\".format(river=river, limit=limit))\n",
    "    else:\n",
    "        logger.debug(\"loading entire df for river {river} from sql\".format(river=river))\n",
    "    con = sqlite3.connect(DATABASE_PATH)\n",
    "    cur = con.cursor()\n",
    "    query = \"\"\"\n",
    "            SELECT timestamp, rain, level, forecast \n",
    "                from {river}\n",
    "            ORDER BY timestamp DESC\n",
    "            LIMIT {limit}\n",
    "        \"\"\"\n",
    "    cur.execute(query.format(river=river, limit=limit))\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    df = pd.DataFrame(result, columns=['timestamp', 'cum_rain', 'level', 'forecast'])\n",
    "    # # Set index to timestamp column as object\n",
    "    df.timestamp = pd.to_datetime(df.timestamp)\n",
    "    df = df.set_index('timestamp')\n",
    "    df = df.sort_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 19:36:58,411 root         DEBUG    loading df for river dart from sql with row limit of 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_level_update_time: 2019-04-12 16:00:00\n",
      "latest_rain_time: 2019-04-12 17:00:00\n",
      "latest_forecast_rain_time: 2019-04-13 21:00:00\n",
      "num_level_updates: 118\n",
      "num_rain_updates: 122\n",
      "num_forecast_rain_updates: 234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testing_mode = False\n",
    "\n",
    "if testing_mode:\n",
    "    current_time = pd.to_datetime(\"2019-04-05 11:30:00\")\n",
    "    df = load_dataframe_from_sql(river=RIVER_NAME, limit=-1)\n",
    "    df = df[df.index > current_time - pd.Timedelta('2days')]\n",
    "    df = df[df.index < current_time + pd.Timedelta('2days')]\n",
    "    df.loc[(df.index > current_time - pd.Timedelta('1days')), \"level\"] = None\n",
    "    df.loc[(df.index > current_time - pd.Timedelta('12hours')), \"cum_rain\"] = None\n",
    "    \n",
    "else:\n",
    "    current_time = time.time()\n",
    "    current_time = pd.to_datetime(current_time - (current_time % (15*60)), unit='s')\n",
    "    df = load_dataframe_from_sql(river=RIVER_NAME, limit=130)\n",
    "\n",
    "\n",
    "latest_level_update_time = max(df[df.level.notnull()].index)\n",
    "latest_rain_time = max(df.index[df.cum_rain.notnull()])\n",
    "latest_forecast_rain_time = max(df.index[df.forecast.notnull()])\n",
    "\n",
    "# Fill in missing timestamps by reindexing\n",
    "min_time = min(df.index)\n",
    "max_time = max(df.index)\n",
    "rng = pd.date_range(min_time, max_time + pd.Timedelta('2.5hours'), freq='15Min')\n",
    "df = df.reindex(rng)\n",
    "\n",
    "num_level_updates = df[df.index <= latest_level_update_time].shape[0]\n",
    "num_rain_updates = df[df.index <= latest_rain_time].shape[0]\n",
    "num_forecast_rain_updates = df[df.index <= latest_forecast_rain_time].shape[0]\n",
    "\n",
    "print \"latest_level_update_time:\", latest_level_update_time\n",
    "print \"latest_rain_time:\", latest_rain_time\n",
    "print \"latest_forecast_rain_time:\", latest_forecast_rain_time\n",
    "\n",
    "print \"num_level_updates:\", num_level_updates\n",
    "print \"num_rain_updates:\", num_rain_updates\n",
    "print \"num_forecast_rain_updates:\", num_forecast_rain_updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/jconn/isthedartrunning/python/production_rnn/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 19:36:59,091 tensorflow   INFO     Restoring parameters from /Users/jconn/isthedartrunning/python/production_rnn/variables/variables\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove rows after latest cum_rain value (no longer using forecast data) \n",
    "df = df[df.index <= latest_rain_time]\n",
    "\n",
    "# Convert cumulative rain to actual rain\n",
    "df['rain'] = df['cum_rain'].diff(periods=2)\n",
    "\n",
    "# negative values from diff are when the rain value resets so we set equal to the cumulative value\n",
    "df.loc[df['rain'] < 0, 'rain'] = df.loc[df['rain'] < 0, 'cum_rain']\n",
    "\n",
    "df['model_rain'] = df[\"rain\"]\n",
    "\n",
    "# Interpolate model_rain\n",
    "\n",
    "df['model_rain'] = df['model_rain'].interpolate()\n",
    "\n",
    "input_df = pd.concat((\n",
    "    df[df.index <=latest_level_update_timestamp].tail(NUM_LEVEL_UPDATES),\n",
    "    df[df.index >latest_level_update_timestamp].tail(NUM_LEVEL_UPDATES)\n",
    "))\n",
    "\n",
    "x = input_df.model_rain.values         \n",
    "y = input_df.level.fillna(0).values\n",
    "timestamps = input_df.index.values\n",
    "\n",
    "# need to padd out input arrays with zeros to get the correct shape\n",
    "num_padding_steps = NUM_STEPS - x.shape[0] \n",
    "x = np.concatenate((x, np.zeros(num_padding_steps)))\n",
    "y = np.concatenate((y, np.zeros(num_padding_steps)))   \n",
    "\n",
    "update_vector = np.zeros(x.shape)\n",
    "update_vector[0:NUM_LEVEL_UPDATES] = 1\n",
    "\n",
    "x = np.column_stack([x, update_vector, update_vector*y])\n",
    "y = np.column_stack([y])\n",
    "\n",
    "model_name = \"production_rnn\"\n",
    "path_to_model = os.path.join(FDIR, model_name)\n",
    "predict_fn = tf.contrib.predictor.from_saved_model(path_to_model)\n",
    "predict = predict_fn({\"x\":[x]})[\"predictions\"]\n",
    "\n",
    "# remove excess padding\n",
    "predict = predict[:-num_padding_steps]\n",
    "level = y[:-num_padding_steps,0]\n",
    "rain = x[:-num_padding_steps,0]\n",
    "\n",
    "# set levels after latest update and predicts before to None\n",
    "level[num_level_updates:] = None\n",
    "predict[:num_level_updates-1] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fn({\"x\":[x]})[\"predictions\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.317, 0.317, 0.317, 0.317, 0.317, 0.317, 0.317, 0.317, 0.317,\n",
       "       0.317, 0.317, 0.317, 0.317, 0.317, 0.317, 0.317, 0.318, 0.318,\n",
       "       0.318, 0.318, 0.317, 0.318, 0.318, 0.317, 0.318, 0.317, 0.318,\n",
       "       0.317, 0.317, 0.317, 0.317, 0.317, 0.316, 0.317, 0.317, 0.317,\n",
       "       0.   , 0.   , 0.   , 0.   ])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 19:37:35,776 root         INFO     currenct level: None\n",
      "2019-04-12 19:37:35,778 root         INFO     OUTPUT TEXT: ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create output json\n",
    "output_df = pd.DataFrame({\"timestamp\":timestamps, \"rain\":rain, \"level\":level, \"predict\": predict})\n",
    "\n",
    "output_df = output_df.round({'level': 3, 'predict': 3, 'model_rain' : 1})\n",
    "output_df = pd.DataFrame(output_df).replace({np.nan:None})\n",
    "\n",
    "\n",
    "if latest_level_update_timestamp == current_time:\n",
    "    current_level = output_df[output_df.timestamp == current_time][\"level\"].values[0]\n",
    "else:\n",
    "    try:\n",
    "        current_level = output_df[output_df.timestamp == current_time][\"predict\"].values[0]\n",
    "    except:\n",
    "        current_level = None\n",
    "\n",
    "logger.info('currenct level: ' + str(current_level))\n",
    "\n",
    "if current_level is None:\n",
    "    text = \"?\"\n",
    "elif current_level > MAXIMUM_THRESHOLD:\n",
    "    text = \"THE DART IS MASSIVE\"\n",
    "elif current_level > MIMIMUM_THRESHOLD:\n",
    "    text = 'YES'\n",
    "elif output_df[output_df.timestamp > current_time][\"predict\"].max() > minimum_threshold:\n",
    "    text = \"THE DART WILL BE UP SHORTLY\"\n",
    "else:\n",
    "    text = 'NO'    \n",
    "\n",
    "logger.info(\"OUTPUT TEXT: \" + text)\n",
    "\n",
    "output_df.timestamp = [timestamp.value / 1000 for timestamp in output_df.timestamp.tolist()]\n",
    "values = output_df.T.to_dict().values()\n",
    "\n",
    "output = {}       \n",
    "output['current_time'] = current_time.value / 1000\n",
    "output['current_level'] = current_level\n",
    "output['text'] = text\n",
    "output['values'] = values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dart.json\", 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
